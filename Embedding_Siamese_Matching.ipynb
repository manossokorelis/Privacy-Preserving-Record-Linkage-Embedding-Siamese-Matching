{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSEl0DyDqUwc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DjbSyG68qas4"
      },
      "outputs": [],
      "source": [
        "file_pathA = '/content/drive/My Drive/THESIS-UOM/DATASETS_KARAKASIDIS/POW_A_10000.csv'\n",
        "file_pathB = '/content/drive/My Drive/THESIS-UOM/DATASETS_KARAKASIDIS/POW_B_1_10000.csv'\n",
        "dataA = pd.read_csv(file_pathA, header=None)\n",
        "dataB = pd.read_csv(file_pathB, header=None)\n",
        "column_namesA = [\"ID_A\", \"Last_Name_A\", \"First_Name_A\", \"Middle_Name_A\", \"Address_A\", \"City_A\", \"Age_A\", \"Race_A\", \"NL_A\", \"Gender_A\"]\n",
        "column_namesB = [\"ID_B\", \"Last_Name_B\", \"First_Name_B\", \"Middle_Name_B\", \"Address_B\", \"City_B\", \"Age_B\", \"Race_B\", \"NL_B\", \"Gender_B\"]\n",
        "dataA.columns = column_namesA\n",
        "dataB.columns = column_namesB\n",
        "dataA = dataA.iloc[:100]\n",
        "dataB = dataB.iloc[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "AAKHvavpq2N-"
      },
      "outputs": [],
      "source": [
        "# Combine first 5 columns into a single text field\n",
        "dataA[\"TextA\"] = dataA.iloc[:, 1:6].astype(str).agg(' '.join, axis=1)\n",
        "dataB[\"TextB\"] = dataB.iloc[:, 1:6].astype(str).agg(' '.join, axis=1)\n",
        "# Load a pre-trained embedding model\n",
        "model = SentenceTransformer('all-MiniLM-L12-v2') # all-MiniLM-L12-v2, paraphrase-MiniLM-L6-v2, roberta-large-nli-stsb-mean-tokens, distilbert-base-nli-stsb-mean-tokens\n",
        "def generate_embeddings_in_batches(data, text_column, batch_size=32):\n",
        "    embeddings = []\n",
        "    for start_idx in range(0, len(data), batch_size):\n",
        "        batch_texts = data[text_column].iloc[start_idx:start_idx+batch_size].tolist()\n",
        "        batch_embeddings = model.encode(batch_texts, batch_size=batch_size)\n",
        "        embeddings.extend(batch_embeddings)\n",
        "    return embeddings\n",
        "# Generate embeddings for dataA and dataB\n",
        "dataA[\"Embedding_A\"] = generate_embeddings_in_batches(dataA, \"TextA\", batch_size=32)\n",
        "dataB[\"Embedding_B\"] = generate_embeddings_in_batches(dataB, \"TextB\", batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "dJrIQTkf3dcF"
      },
      "outputs": [],
      "source": [
        "def normalize_embeddings(embeddings):\n",
        "    mean = np.mean(embeddings, axis=0)\n",
        "    std_dev = np.std(embeddings, axis=0)\n",
        "    normalized_embeddings = (embeddings - mean) / (std_dev + 1e-6)  # Add small epsilon to avoid division by zero\n",
        "    return normalized_embeddings\n",
        "def add_dp_noise_to_embeddings(embeddings, epsilon, sensitivity):\n",
        "    # Normalize the embeddings before adding noise\n",
        "    normalized_embeddings = normalize_embeddings(embeddings)\n",
        "    # Calculate the standard deviation for the noise\n",
        "    std_dev = sensitivity / epsilon  # Sensitivity / Privacy Budget\n",
        "    # Generate Gaussian noise\n",
        "    noise = np.random.normal(loc=0.0, scale=std_dev, size=normalized_embeddings.shape)\n",
        "    # Add noise to the normalized embeddings\n",
        "    noisy_embeddings = normalized_embeddings + noise\n",
        "    return noisy_embeddings\n",
        "# Adds Gaussian noise for Differential Privacy to the embeddings\n",
        "dataA[\"Embedding_A\"] = dataA[\"Embedding_A\"].apply(lambda x: add_dp_noise_to_embeddings(x, epsilon=10.0, sensitivity=1.0))\n",
        "dataB[\"Embedding_B\"] = dataB[\"Embedding_B\"].apply(lambda x: add_dp_noise_to_embeddings(x, epsilon=10.0, sensitivity=1.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PZiisQLoOdP_"
      },
      "outputs": [],
      "source": [
        "def creatingPairs_chunked(dataA, dataB, chunk_size=1000):\n",
        "    pairs_list = []\n",
        "    for i in range(0, len(dataA), chunk_size):\n",
        "        chunkA = dataA.iloc[i:i+chunk_size]\n",
        "        for j in range(0, len(dataB), chunk_size):\n",
        "            chunkB = dataB.iloc[j:j+chunk_size]\n",
        "            pairs = chunkA.assign(key=1).merge(chunkB.assign(key=1), on='key').drop('key', axis=1)\n",
        "            pairs['Matched'] = (pairs['ID_A'] == pairs['ID_B']).astype(int)\n",
        "            pairs_list.append(pairs[['Embedding_A', 'Embedding_B', 'Matched']])\n",
        "    pairs = pd.concat(pairs_list, ignore_index=True)\n",
        "    return pairs\n",
        "# Generating record pairs\n",
        "pairs = creatingPairs_chunked(dataA=dataA, dataB=dataB)\n",
        "train_pairs, test_pairs = train_test_split(pairs, test_size=0.2, random_state=42)\n",
        "train_pairs, valid_pairs = train_test_split(train_pairs, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3rbVcSdGbR08"
      },
      "outputs": [],
      "source": [
        "class EmbeddingDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        emb_a = torch.tensor(self.data.iloc[idx][\"Embedding_A\"], dtype=torch.float32)\n",
        "        emb_b = torch.tensor(self.data.iloc[idx][\"Embedding_B\"], dtype=torch.float32)\n",
        "        label = torch.tensor(self.data.iloc[idx][\"Matched\"], dtype=torch.float32)\n",
        "        return emb_a, emb_b, label\n",
        "# Create datasets\n",
        "train_dataset = EmbeddingDataset(train_pairs)\n",
        "valid_dataset = EmbeddingDataset(valid_pairs)\n",
        "test_dataset = EmbeddingDataset(test_pairs)\n",
        "# Create loaders\n",
        "train_loader = DataLoader(train_dataset, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "sdRl7GDr95WA"
      },
      "outputs": [],
      "source": [
        "class SiameseNN(nn.Module):\n",
        "    def __init__(self, input_dim=384):\n",
        "        super(SiameseNN, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),\n",
        "        )\n",
        "    def forward(self, emb_a, emb_b):\n",
        "        diff = torch.abs(emb_a - emb_b)\n",
        "        return self.fc(diff)\n",
        "# Initialize Siamese Model, Loss, Optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SiameseNN().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=np.array(train_pairs[\"Matched\"]))\n",
        "pos_weight = torch.tensor(class_weights[1], dtype=torch.float32).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_TVW35XyJCL",
        "outputId": "0c4e57c0-dd6f-4c02-b4f6-74279648822b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5: Loss 0.8177, Accuracy 0.9894\n",
            "Epoch 2/5: Loss 0.6006, Accuracy 0.9894\n",
            "Epoch 3/5: Loss 0.4348, Accuracy 0.9900\n",
            "Epoch 4/5: Loss 0.2513, Accuracy 0.9962\n",
            "Epoch 5/5: Loss 0.1075, Accuracy 0.9938\n"
          ]
        }
      ],
      "source": [
        "def train_model(model, train_loader, valid_loader, criterion, optimizer, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for emb_a, emb_b, labels in train_loader:\n",
        "            emb_a, emb_b, labels = emb_a.to(device), emb_b.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(emb_a, emb_b)\n",
        "            loss = criterion(outputs.squeeze(), labels.squeeze())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for emb_a, emb_b, labels in valid_loader:\n",
        "                emb_a, emb_b, labels = emb_a.to(device), emb_b.to(device), labels.to(device)\n",
        "                outputs = model(emb_a, emb_b)\n",
        "                preds = (outputs > 0.5).float()\n",
        "                correct += (preds.squeeze() == labels.squeeze()).sum().item()\n",
        "                total += labels.size(0)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}: Loss {total_loss / len(train_loader):.4f}, Accuracy {correct / total:.4f}\")\n",
        "# Train the Model\n",
        "train_model(model, train_loader, valid_loader, criterion, optimizer, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "xfPlHYGAyK_j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41539835-e95e-4017-d6c0-ca1227fb2b3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9910\n",
            "Test Precision: 0.8889\n",
            "Test Recall: 0.3200\n",
            "Test F1-score: 0.4706\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for emb_a, emb_b, labels in test_loader:\n",
        "            emb_a, emb_b, labels = emb_a.to(device), emb_b.to(device), labels.to(device).unsqueeze(1)\n",
        "            outputs = model(emb_a, emb_b)\n",
        "            preds = (outputs > 0.5).float()\n",
        "            all_preds.extend(preds.cpu().numpy().flatten())\n",
        "            all_labels.extend(labels.cpu().numpy().flatten())\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds)\n",
        "    recall = recall_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds)\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Test Precision: {precision:.4f}\")\n",
        "    print(f\"Test Recall: {recall:.4f}\")\n",
        "    print(f\"Test F1-score: {f1:.4f}\")\n",
        "# Test the Model\n",
        "evaluate_model(model, test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}